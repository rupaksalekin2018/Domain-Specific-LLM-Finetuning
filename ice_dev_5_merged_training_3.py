# -*- coding: utf-8 -*-
"""ice_dev_5_merged_training_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ebjH1N9-UAVhlZZXA-GRc4aJ5AHjy1WC
"""

# import pandas as pd

# # Load the CSV file
# file_path = '/content/drive/MyDrive/dataset/corrected_augmented_merged_training_data.csv'  # Replace with the path to your CSV file
# df = pd.read_csv(file_path)

# # Step 1: Remove empty columns
# df.dropna(axis=1, how='all', inplace=True)

# # Step 2: Fix the 'id' column with a new order
# df['id'] = range(1, len(df) + 1)

# # Step 3: Save the cleaned-up dataframe to a new CSV file
# output_file_path = '/content/drive/MyDrive/dataset/corrected_augmented_training_data_final_check_fixed.csv'  # Replace with your desired output file path
# df.to_csv(output_file_path, index=False)

# print(f"Cleaned dataset saved to {output_file_path}")

# import pandas as pd

# # Load the CSV file
# file_path = '/content/drive/MyDrive/dataset/corrected_augmented_training_data_final_check_fixed.csv'  # Replace with the path to your CSV file
# df = pd.read_csv(file_path)

# # Step 1: Remove empty columns
# df.dropna(axis=1, how='all', inplace=True)

# # Step 2: Fix the 'id' column with a new order
# df['id'] = range(1, len(df) + 1)

# # Step 3: Save the cleaned-up dataframe to a new CSV file
# output_file_path = '/content/drive/MyDrive/dataset/corrected_augmented_training_data_final_fixed.csv'  # Replace with your desired output file path
# df.to_csv(output_file_path, index=False)

# print(f"Cleaned dataset saved to {output_file_path}")

!pip install --upgrade pyarrow
# or
!pip uninstall pyarrow -y
!pip install pyarrow

import pandas as pd

# Load the CSV file into a DataFrame
csv_file_path = '/content/drive/MyDrive/dataset/corrected_augmented_training_data_final_check_fixed.csv'
df = pd.read_csv(csv_file_path)

# Ensure the data has a "train" mapping, add a 'train' column with 'train' value
df['train'] = 'train'

# Create a new 'conversations' column as a list of dictionaries
df['conversations'] = df.apply(lambda row: [
    {"from": "system", "value": "You are an assistant that will help to find the exact & appropriate answer for the given question"},
    {"from": "human", "value": row['Question']},
    {"from": "gpt", "value": row['Answer']}
], axis=1)

# Remove the 'id', 'Question', and 'Answer' columns
df = df.drop(columns=['id', 'Question', 'Answer'])

# Display the updated DataFrame
print(df.head())

df.head()

df['conversations'][0]

df['conversations'][100]

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Installs Unsloth, Xformers (Flash Attention) and all other packages!
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# 
# # We have to check which Torch version for Xformers (2.3 -> 0.0.27)
# from torch import __version__; from packaging.version import Version as V
# xformers = "xformers==0.0.27" if V(__version__) < V("2.4.0") else "xformers"
# !pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton

!pip uninstall unsloth -y
!pip install --upgrade --force-reinstall --no-cache-dir git+https://github.com/unslothai/unsloth.git

import unsloth
from unsloth import FastLanguageModel
from unsloth import is_bfloat16_supported

import torch

max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

df.info()

df.tail()

# df[id]

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

# Define formatting function
def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = []

    for convo in convos:
        try:
            # Ensure convo is a list of messages (if that is the structure)
            if isinstance(convo, list):
                # Apply the tokenizer's chat template
                text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)

                # Check if text is a list and join it into a single string if necessary
                if isinstance(text, list):
                    text = " ".join([str(t) for t in text])  # Ensure all elements are strings before joining
            else:
                # If it's not a list, handle it directly
                text = str(convo)

            texts.append(text)
        except TypeError:
            # Handle the case where something goes wrong (fallback)
            texts.append(str(convo))  # Convert to string and append

    return {"text": texts}

import datasets

# Convert DataFrame to Dataset if needed
dataset = datasets.Dataset.from_pandas(df)

# Apply standardization from Unsloth
from unsloth.chat_templates import standardize_sharegpt
dataset = standardize_sharegpt(dataset)

# Apply the formatting prompts function
dataset = dataset.map(formatting_prompts_func, batched=True)

dataset[5]["text"]

dataset[5]["conversations"]

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference
def generate_text(text):
    inputs = tokenizer(text, return_tensors="pt").to("cuda:0")

    # text_streamer = TextStreamer(tokenizer)

    outputs = model.generate(**inputs,
                             # streamer=text_streamer,
                             max_new_tokens=256)

    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

print("Before training \n")
generate_text('FLANGE WELDNECK CS A105N FORGED 10" CLS 600 WT 11.12MM RF ASME B16.5')

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False,  # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 25,
        num_train_epochs = 2,  # Set this for 1 full training run.
        max_steps = 4000,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    )
)

from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)

tokenizer.decode(trainer.train_dataset[5]["input_ids"])

space = tokenizer(" ", add_special_tokens = False).input_ids[0]
tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5]["labels"]])

import os
import wandb

# Initialize wandb with a project name and any other desired configurations.
wandb.init(project="my-awesome-project")

# Previous code remains the same:
os.environ["WANDB_DISABLED"] = "False"

trainer_stats = trainer.train()

model.save_pretrained_gguf("dir_1", tokenizer, quantization_method = "q4_k_m")

model.save_pretrained_gguf("dir_2", tokenizer, quantization_method = "f16")

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : PIPE SS A358 TP316/316L CL1 EFW 20" SCH 80S SRL BE ASME B36.19M'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : PIPE SS A358 TP316/316L CL1 EFW 20" SCH 80S SRL BE ASME B36.19M'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : PIPE'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : PIPE'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : TP316/316L'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : TP316/316L'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

model.save_pretrained_gguf("ice_dev_q4_dir", tokenizer, quantization_method = "q4_k_m")

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : DRL'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : DRL'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : A53/A106/API 5L ASME B36.10M'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : A53/A106/API 5L ASME B36.10M'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : TA 2.77'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : TA 2.77'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : WELD ASME B36.10M'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : 355.6'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : 355.6'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : 80S'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : 80S'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : UNS S31803'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : UNS S31803'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question : UNS C71500'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question, if you do not find any, then just say sorry no match : UNS C71500'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question, if you do not find any, then just say sorry no match : X80 UNS C71500'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question, if you do not find any, then just say sorry no match : Gasket'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question, also try to find the single entities even if thats a single input, if you do not find any, then just say sorry no match : Gasket'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question, also try to find the single entities even if thats a single input, if you do not find any, then just say sorry no match : Valve'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': 'you do not have to be creative, just give me the exact answer with you are trained on for this question, also try to find the single entities even if thats a single input, if you do not find any, then just say sorry no match : Pipe'},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)

FastLanguageModel.for_inference(model)  # Enable native 2x faster inference

messages = [
    {'role': 'user', 'content': '''you do not have to be creative,
     just give me the exact answer, Try to find only the entities,
      also try to find the single entities even if thats a single input,
      if you do not find any, then just say sorry no match, no additional information is needed, Find for given value : Gasket'''},
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # Must add for generation
    return_tensors="pt",
).to("cuda")

outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True,
                         temperature=0.1, min_p=0.1)

tokenizer.batch_decode(outputs)