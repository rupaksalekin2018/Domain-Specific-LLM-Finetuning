# -*- coding: utf-8 -*-
"""ice_dev_5_merged_training_dataset_preparation_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfmRB5WUZ7pY1Mb0K0yIIMpmQ8plpstd
"""

import pandas as pd
from itertools import combinations

# Load the dataset
file_path = '/content/drive/MyDrive/dataset/pipe_small_dataset.xlsx'  # Replace with your file path
dataset = pd.read_excel(file_path)

# Load the dataset
file_path = '/content/drive/MyDrive/dataset/fitting_small_dataset.csv'  # Replace with your file path
dataset = pd.read_csv(file_path)

# Function to generate all combinations of column-value pairs for a row
def generate_all_combinations(row, columns):
    """
    Generate all possible combinations of column values for a row.
    For each combination, create a question and answer pair.
    """
    question_answer_pairs = []

    # Loop over combinations of 1, 2, 3, ... up to all columns
    for i in range(1, len(columns) + 1):
        # Get all combinations of columns of size 'i'
        for combo in combinations(columns, i):
            question_parts = []  # List to hold the parts of the question
            answer_parts = []    # List to hold the parts of the answer

            # Loop over each column in the current combination
            for col in combo:
                # Get the value from the row for the current column
                value = row[col]

                # Check if the value is not null
                if pd.notnull(value):
                    # Add the value to the question list
                    question_parts.append(str(value))

                    # Add the 'COLUMN_NAME: value' format to the answer list
                    answer_parts.append(f"{col}: {value}")

            # Join the question and answer parts with spaces and commas, respectively
            question = " ".join(question_parts)
            answer = ", ".join(answer_parts)

            # Add the complete question-answer pair to the list
            question_answer_pairs.append((question, answer))

    return question_answer_pairs  # Return all generated question-answer pairs

# Initialize a list to store data for all rows
all_combinations_list = []

# Process each row in the dataset
for row_index, row in dataset.iterrows():
    # Generate all combinations for the current row
    combinations_list = generate_all_combinations(row, dataset.columns)

    # Create an ID for each combination and add to the list
    for idx, (question, answer) in enumerate(combinations_list):
        all_combinations_list.append(
            {"id": f"{row_index + 1}_{idx + 1}", "Question": question, "Answer": answer}
        )

# Convert the list of dictionaries into a DataFrame
all_combinations_data = pd.DataFrame(all_combinations_list)

"""check the error for large data which is out of bound for sheet"""

# # Save the resulting dataset to a new Excel file
# output_file = '/content/drive/MyDrive/dataset/all_combinations_for_fitting_all_rows.xlsx'  # Replace with your desired path
# all_combinations_data.to_excel(output_file, index=False)

# print(f"Combinations file saved to {output_file}")

# Save the resulting dataset to a new Excel file
output_file = '/content/drive/MyDrive/dataset/all_combinations_for_fitting_all_rows.csv'  # Replace with your desired path
all_combinations_data.to_csv(output_file, index=False)

print(f"Combinations file saved to {output_file}")

!pip install transformers nltk nlpaug augly

import nltk
from nltk.corpus import wordnet
import random
import pandas as pd
from transformers import pipeline, MarianMTModel, MarianTokenizer
import nlpaug.augmenter.word as naw
import augly.text as txt

# Download necessary NLTK data
nltk.download('wordnet')
nltk.download('omw-1.4')

# # Load dataset (replace 'fixed_merged_training_data.csv' with your file path)
# df = pd.read_excel('/content/drive/MyDrive/dataset/all_combinations_for_fitting_all_rows.xlsx')

# Load dataset (replace 'fixed_merged_training_data.csv' with your file path)
df = pd.read_csv('/content/drive/MyDrive/dataset/all_combinations_for_fitting_all_rows.csv')

# Function for Synonym Replacement
def synonym_replacement(sentence, n=1):
    words = sentence.split()
    new_words = words.copy()
    for _ in range(n):
        word_to_replace = random.choice(new_words)
        synonyms = wordnet.synsets(word_to_replace)
        if synonyms:
            lemmas = synonyms[0].lemmas()
            new_word = random.choice(lemmas).name()
            new_words = [new_word if word == word_to_replace else word for word in new_words]
    return ' '.join(new_words)

# Function for Random Swap
def random_swap(sentence, n=1):
    words = sentence.split()
    for _ in range(n):
        idx1, idx2 = random.sample(range(len(words)), 2)
        words[idx1], words[idx2] = words[idx2], words[idx1]
    return ' '.join(words)

# Function for Random Deletion
def random_deletion(sentence, p=0.1):
    words = sentence.split()
    if len(words) == 1:
        return sentence
    new_words = [word for word in words if random.uniform(0, 1) > p]
    return ' '.join(new_words) if new_words else random.choice(words)

# Combined Augmentation Function
def combined_augmentation(sentence):
    # Apply synonym replacement
    sentence = synonym_replacement(sentence, n=2)
    # Apply random swap
    sentence = random_swap(sentence, n=1)
    # Apply random deletion
    sentence = random_deletion(sentence, p=0.1)
    return sentence

# Example usage of combined augmentation
sentence = "This is a sample sentence for augmentation."
augmented_sentence = combined_augmentation(sentence)
print("Original:", sentence)
print("Augmented:", augmented_sentence)

augmented_data = []
for _, row in df.iterrows():
    original_question = row['Question']

    # Check if the question is a valid string
    if isinstance(original_question, str):
        # Try to generate augmented questions
        augmented_questions = []
        try:
            augmented_questions = [combined_augmentation(original_question) for _ in range(10)]
        except ValueError as e:
            print(f"Augmentation failed for question: {original_question} with error: {e}")
            augmented_questions = [original_question]  # Use original question as a fallback
    else:
        print(f"Invalid question (not a string): {original_question}")
        augmented_questions = [str(original_question)]  # Convert NaN/None to string if necessary

    # Append the augmented data
    for augmented_question in augmented_questions:
        augmented_data.append({
            'id': row['id'],
            'Question': augmented_question,
            'Answer': row['Answer']
        })

# Convert to DataFrame and save
augmented_df = pd.DataFrame(augmented_data)
augmented_df.to_csv('augmented_fitting_training_data.csv', index=False)

#avoid gasket.

augmented_df.info()

augmented_df.head()

augmented_df.tail()

from google.colab import files
files.download('augmented_training_data.csv')

# Load the augmented dataset
augmented_df = pd.read_csv('augmented_fitting_training_data.csv')

# Fix the id column to be sequential starting from 1
augmented_df['id'] = range(1, len(augmented_df) + 1)

# Save the corrected dataframe
augmented_df.to_csv('corrected_augmented_fitting_training_data.csv', index=False)

# Download the corrected dataset
from google.colab import files
files.download('corrected_augmented_fitting_training_data.csv')

import pandas as pd

# Prompt user to input the full paths for four CSV files
file_paths = ['/content/drive/MyDrive/dataset/corrected_augmented_training_data_final_only_for_pipe.csv', '/content/drive/MyDrive/dataset/corrected_augmented_valve_training_data.csv', '/content/drive/MyDrive/dataset/corrected_augmented_gasket_training_data.csv']
# for i in range(4):
#     file_path = input(f"Enter the full path for file {i+1} (e.g., /path/to/file.csv): ")
#     file_paths.append(file_path)

# Initialize an empty list to hold DataFrames
dataframes = []

# Loop through each file path, read it into a DataFrame, and append to the list
for path in file_paths:
    df = pd.read_csv(path)
    dataframes.append(df)

# Concatenate all DataFrames into one
merged_df = pd.concat(dataframes, ignore_index=True)

# Save the merged DataFrame to a new CSV file
output_path = "/content/drive/MyDrive/dataset/corrected_augmented_merged_training_data.csv"
merged_df.to_csv(output_path, index=False)

print(f"Merged CSV file saved at: {output_path}")